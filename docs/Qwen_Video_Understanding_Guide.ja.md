- **あなたの現状の実装は非常に高度です**  
  - 動画フレームに「絶対時間」と「取得開始からの経過秒数」を埋め込み済み  
  - シーン変化検出ロジック（閾値ベース）により、シーン区切りのメタデータ生成が可能  
  - NAudio による音声ストリーム取得 + Whisper.net によるリアルタイム文字起こしを実装済み  

- **しかし、LLMが「スムーズに理解しない」根本原因は以下の通り**  
  - LLM（Qwen-VLなど）は、単なる画像を「静止画」としてしか処理せず、「動画の一部」という文脈を自動で推論できない  
  - フレームが独立して送信されている場合、LLMは「連続性」や「変化」を認識できない  
  - 画像のみでは、テキスト・オブジェクト・動作の意図がぼやけて識別不能になる（特に低解像度時）  

- **kmizu/embodied-claude の成功要因（参考）**  
  - 入力は「1フレームの画像 ＋ 前後5秒の字幕テキスト ＋ 現在時刻」の組み合わせ  
  - プロンプトに明示的に「前回の状態：〇〇でした。現在のフレームでは？」と文脈を埋め込んでいる  
  - 画像には特にオーバーレイは不要だが、**言語による文脈提示が鍵**  

- **即効性のある改善策（今すぐ実施可能）**  
  - 【必須】プロンプトを「時系列文脈付き」に改修  
    - 例: `"現在時刻: 2026-02-15T23:05:12Z。前回（00:05:08）のフレームでは「人物が玄関に立っていました」。今回のフレームでは何が起こっていますか？"`  
  - 【推奨】フレーム画像に軽い視覚的ヒントを追加（C# + SkiaSharp / System.Drawing）  
    - 例: 左上に `[TS:00:02:17]`、中央に `[SCENE CHANGE]` を白文字で描画  
    - → LLMの注意を「時間」「変化」へ誘導できる  
  - 【有効】2フレームを横並びにした「差分画像」を入力として与える  
    - 例: `frame_t.png` と `frame_t+1.png` を左右に連結 → LLMが「動き」を直接見られる  
  - 【補助】重要フレームのみを選別（モーション検出 or キーフレーム抽出）→ 16枚以内に収める  

- **重要な制約の再確認**  
  - Qwen-VL の画像入力上限は **16枚**（それ以上はエラーまたは無視）  
  - 動画全体を送るのではなく、「文脈＋重要フレーム＋字幕」の最小集合で伝えるのが現実的  
  - 音声テキストは「字幕」としてプロンプトに埋め込むのが最も効果的（画像内OCRより信頼性高い）  

- **結論**  
  - 技術基盤（RTSP→フレーム→音声→メタデータ）は完成しています  
  - 欠けているのは「LLMの認知特性に合わせた提示設計」のみ  
  - 「どう見せるか」が「どう理解させるか」を決める — これが最大のポイントです