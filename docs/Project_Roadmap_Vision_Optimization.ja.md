Project Roadmap: LLM-Friendly Frame Optimization for MCP Server

Purpose:
Windows Desktop Use MCP サーバを、将来的に Claude Desktop や他の汎用 LLM クライアントが実際に利用できるよう、
「画像＋文脈」の提示方法を体系的に最適化する計画。

Note:
- 本プロジェクトの最終的なユーザーは、**Vision専用モデルではなく、Claude / Qwen / GPT 等の汎用 LLM クライアント**（例: Claude Desktop）です。
- そのため、すべての設計は「モデルに依存しない」ことを前提とします。
- Visionモデル（qwencode）は「実装支援者」であり、最終的な出力は汎用性を保つ必要があります。

1. 現状と課題
- 技術基盤は整備済み：RTSP/カメラ → フレーム抽出 + 時間メタデータ + Whisper音声文字起こし
- 問題点: LLMが画像を「孤立した静止画」として解釈し、文脈・変化・時間軸を捉えられない
- 特に汎用モデルは、視覚情報より言語文脈に依存するため、「画像に何が書かれているか」より「プロンプトに何が書かれているか」が重要

2. 設計原則（汎用モデル対応）
- Principle 1: 「画像は補助情報」、主役は「言語文脈」
  - 画像には最低限の視覚ヒント（タイムコード、シーンラベル）のみを埋め込む
  - 詳細な解釈は、プロンプト内のテキストで提供する
- Principle 2: 入力フォーマットは JSON Schema で統一
  - 例: { "frame_b64": "...", "timestamp": "2026-02-15T23:05:12Z", "prev_summary": "人物が玄関に立っていました", "subtitle": "次は東京駅へ移動します" }
  - これにより、Claude / Qwen / GPT いずれも同じ構造で処理可能
- Principle 3: 画像加工は「破壊的でない」範囲に留める
  - オーバーレイは白文字・12px・左上固定（背景色に依存しない）
  - 差分画像は横並び but 両フレームを等倍表示（拡大縮小は避ける）
  - 一切のフィルター（エッジ強調・コントラスト調整）はオプションで、デフォルトは無加工

3. 役割分担（モデル非依存型）
- qwencode（Qwen-VL）:  
  - 実装支援者として、**「LLMが読みやすい画像生成ロジック」の草案**を提供  
  - 但し、出力コードは「System.Drawing.Common による軽量加工」に限定し、SkiaSharpやOpenCVは使用しない  
  - コード内に「この加工は汎用モデルでも有効」という意図をコメントで明記

- opencode（開発者＝あなた）:  
  - 加工ロジックを `ScreenCaptureService.cs` に組み込み、CLIオプションで切り替え可能にする  
    • `--context-mode full`（デフォルト）: タイムコード＋字幕＋前回要約をプロンプトに埋め込む  
    • `--overlay minimal`: 左上に `[TS:00:02:17]` のみオーバーレイ  
    • `--diff none`: 差分画像無効（汎用モデル向けデフォルト）  
  - 出力は常に `FrameContext.cs` オブジェクト経由で統一フォーマット生成

- Gemini（監査役）:  
  - 汎用モデルとの互換性を確認（例: Claude Desktop の入力制限チェック）  
  - JSONスキーマの保守性・拡張性をレビュー  

4. 実装フェーズ計画（モデル非依存版）
| フェーズ | タスク | 重点 |
|----------|--------|------|
| Phase 0 | ドキュメント整備（本件） | 汎用性を明示 |
| Phase 1 | qwencode に「最小限のオーバーレイ＋文脈付きプロンプト生成」ロジックを依頼 | Visionモデル活用 but 汎用指向 |
| Phase 2 | opencode が CLI オプションと FrameContext 構造体を実装 | JSON出力統一 |
| Phase 3 | Claude Desktop で実際のMCP呼び出しテスト | 汎用モデル動作確認 |
| Phase 4 | ユーザー定義プロンプトテンプレート対応 | `--prompt-template path.json` |

5. 成功指標（汎用モデル基準）
- Claude Desktop が「前回と今回の違い」を自然言語で正確に説明できる  
- 同一フレームを異なるモデル（Qwen-VL / Claude 3.5 / GPT-4o）で送信した場合、解釈の不一致率 < 15%  
- プロンプトなしで画像だけ送った場合と、文脈付きで送った場合の理解精度差が +40% 以上

6. 備考
- 「vision-model専用機能」は一切実装しない。必要なら別プロジェクトで分離。
- すべてのコード・ドキュメントは英語コメント（日本語はドキュメントのみ）。